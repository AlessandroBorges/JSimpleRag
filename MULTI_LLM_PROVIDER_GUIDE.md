# Guia de M√∫ltiplos Provedores LLM

**Data**: 2025-10-13
**Status**: ‚úÖ Implementado

---

## Vis√£o Geral

O JSimpleRag suporta configura√ß√£o de m√∫ltiplos provedores LLM (prim√°rio e secund√°rio) com estrat√©gias flex√≠veis de uso:

- **Backup/Failover**: Provedor secund√°rio como backup quando o prim√°rio falha
- **Load Balancing**: Distribui√ß√£o de carga entre provedores
- **Especializa√ß√£o**: Provedores diferentes para tarefas diferentes
- **Verifica√ß√£o Dupla**: Valida√ß√£o cruzada de resultados
- **Roteamento Inteligente**: Escolha autom√°tica baseada na complexidade

---

## Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    LLMServiceManager (Bean)         ‚îÇ
‚îÇ  - Gerencia estrat√©gias             ‚îÇ
‚îÇ  - Failover autom√°tico              ‚îÇ
‚îÇ  - Estat√≠sticas de uso              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ             ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇPrimary ‚îÇ   ‚îÇSecondary‚îÇ
‚îÇProvider‚îÇ   ‚îÇProvider‚îÇ
‚îÇ(Local) ‚îÇ   ‚îÇ(Cloud) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Configura√ß√£o

### application.properties

```properties
# ======================================
# Primary Provider (LM Studio Local)
# ======================================
llmservice.provider.name=LM_STUDIO
llmservice.provider.llm.models=qwen/qwen3-1.7b
llmservice.provider.embedding.model=text-embedding-nomic-embed-text-v1.5@q8_0
llmservice.provider.embedding.dimension=768
llmservice.provider.api.url=http://localhost:1234/v1
llmservice.provider.api.key=

# ======================================
# Secondary Provider (OpenAI Cloud)
# ======================================
llmservice.provider2.enabled=true
llmservice.provider2.name=OPENAI
llmservice.provider2.llm.models=gpt-3.5-turbo,gpt-4
llmservice.provider2.embedding.model=text-embedding-ada-002
llmservice.provider2.embedding.dimension=1536
llmservice.provider2.api.url=https://api.openai.com/v1
llmservice.provider2.api.key=${OPENAI_API_KEY}

# ======================================
# Strategy Configuration
# ======================================
llmservice.strategy=FAILOVER
llmservice.failover.max-retries=3
llmservice.failover.timeout-seconds=30
```

### Vari√°veis de Ambiente (.env)

```bash
# Primary Provider
LLM_PROVIDER_CLASS=LM_STUDIO
LLM_MODELS=qwen/qwen3-1.7b
EMBEDDING_MODEL=text-embedding-nomic-embed-text-v1.5@q8_0
EMBEDDING_DIMENSION=768
LLM_API_URL=http://localhost:1234/v1

# Secondary Provider
LLM_PROVIDER2_ENABLED=true
LLM_PROVIDER2_CLASS=OPENAI
LLM_PROVIDER2_MODELS=gpt-3.5-turbo
LLM_PROVIDER2_EMBEDDING_MODEL=text-embedding-ada-002
LLM_PROVIDER2_EMBEDDING_DIMENSION=1536
LLM_PROVIDER2_API_URL=https://api.openai.com/v1
LLM_PROVIDER2_API_KEY=sk-...

# Strategy
LLM_STRATEGY=FAILOVER
LLM_MAX_RETRIES=3
LLM_TIMEOUT_SECONDS=30
```

---

## Estrat√©gias Dispon√≠veis

### 1. PRIMARY_ONLY
**Uso**: Apenas o provedor prim√°rio, secund√°rio ignorado

```properties
llmservice.strategy=PRIMARY_ONLY
```

**Caso de Uso**:
- Ambiente de desenvolvimento
- Testes locais
- Custos m√≠nimos

**Comportamento**:
```
Request ‚Üí Primary Provider ‚Üí Response
```

---

### 2. FAILOVER (Padr√£o)
**Uso**: Prim√°rio com backup autom√°tico

```properties
llmservice.strategy=FAILOVER
llmservice.failover.max-retries=3
```

**Caso de Uso**:
- Alta disponibilidade
- Backup autom√°tico
- Produ√ß√£o confi√°vel

**Comportamento**:
```
Request ‚Üí Primary Provider
   ‚îú‚îÄ Success ‚Üí Response
   ‚îî‚îÄ Failure ‚Üí Secondary Provider
       ‚îú‚îÄ Success ‚Üí Response
       ‚îî‚îÄ Failure ‚Üí Exception
```

**Exemplo de Uso**:
```java
@Service
public class DocumentService {

    @Autowired
    private LLMServiceManager llmManager;

    public float[] generateEmbedding(String text) {
        // Tenta prim√°rio, faz failover se falhar
        return llmManager.generateEmbedding(text);
    }
}
```

---

### 3. ROUND_ROBIN
**Uso**: Distribui√ß√£o alternada entre provedores

```properties
llmservice.strategy=ROUND_ROBIN
```

**Caso de Uso**:
- Balanceamento de carga
- Distribui√ß√£o de custos
- Evitar rate limits

**Comportamento**:
```
Request 1 ‚Üí Primary Provider
Request 2 ‚Üí Secondary Provider
Request 3 ‚Üí Primary Provider
Request 4 ‚Üí Secondary Provider
...
```

**Estat√≠sticas Esperadas**:
```
Primary: 50% das requisi√ß√µes
Secondary: 50% das requisi√ß√µes
```

---

### 4. SPECIALIZED
**Uso**: Provedores especializados por tipo de opera√ß√£o

```properties
llmservice.strategy=SPECIALIZED
```

**Caso de Uso**:
- Embeddings locais (r√°pido) + Completions cloud (qualidade)
- Modelos especializados por tarefa

**Comportamento**:
```
Embedding Request ‚Üí Primary Provider (fast local)
Completion Request ‚Üí Secondary Provider (powerful cloud)
```

**Exemplo de Configura√ß√£o**:
```properties
# Primary: Local para embeddings r√°pidos
llmservice.provider.name=LM_STUDIO
llmservice.provider.embedding.model=nomic-embed-text

# Secondary: Cloud para completions complexos
llmservice.provider2.name=OPENAI
llmservice.provider2.llm.models=gpt-4
```

---

### 5. DUAL_VERIFICATION
**Uso**: Executa em ambos e compara resultados

```properties
llmservice.strategy=DUAL_VERIFICATION
```

**Caso de Uso**:
- Quality assurance
- A/B testing
- Valida√ß√£o de consist√™ncia

**Comportamento**:
```
Request ‚Üí Primary Provider   ‚îÄ‚îê
       ‚Üí Secondary Provider ‚îÄ‚î§
                             ‚îú‚îÄ Compare ‚Üí Response
                             ‚îÇ  (log similarity)
```

**Custo**: 2x (executa em ambos)

**Exemplo de Log**:
```
INFO  LLMServiceManager - Dual verification: Vector similarity = 0.94
WARN  LLMServiceManager - Dual verification: Low similarity (0.65)
```

---

### 6. SMART_ROUTING
**Uso**: Roteamento baseado na complexidade

```properties
llmservice.strategy=SMART_ROUTING
```

**Caso de Uso**:
- Otimiza√ß√£o de custos
- Performance h√≠brida
- Ambiente misto local/cloud

**Comportamento**:
```
Simple Query (< 1000 chars) ‚Üí Primary (fast, free)
Complex Query (> 1000 chars) ‚Üí Secondary (powerful, paid)

Keywords "explain", "analyze" ‚Üí Secondary
Normal queries ‚Üí Primary
```

**Heur√≠sticas**:
- Comprimento do texto
- Palavras-chave complexas
- Tipo de opera√ß√£o

---

### 7. MODEL_BASED ‚≠ê **NOVO**
**Uso**: Roteamento autom√°tico pelo nome do modelo

```properties
llmservice.strategy=MODEL_BASED
```

**Caso de Uso**:
- M√∫ltiplos provedores com modelos diferentes
- Roteamento transparente baseado no modelo solicitado
- Aplica√ß√µes que usam modelos espec√≠ficos dinamicamente

**Comportamento**:
```
Request com model="gpt-4" ‚Üí Busca provedor com GPT-4
Request com model="llama2" ‚Üí Busca provedor com Llama2
Request com model="mistral" ‚Üí Busca provedor com Mistral

Se modelo n√£o encontrado ‚Üí Fallback para Primary
```

**Exemplo de Configura√ß√£o**:
```properties
# Primary: LM Studio com modelos locais
llmservice.provider.name=LM_STUDIO
llmservice.provider.llm.models=llama2,mistral,qwen

# Secondary: OpenAI com modelos cloud
llmservice.provider2.enabled=true
llmservice.provider2.name=OPENAI
llmservice.provider2.llm.models=gpt-3.5-turbo,gpt-4

# Estrat√©gia
llmservice.strategy=MODEL_BASED
```

**Uso no C√≥digo**:
```java
@Service
public class ChatService {
    @Autowired
    private LLMServiceManager llmManager;

    public String chat(String message, String preferredModel) {
        // Automaticamente roteia para o provedor correto
        return llmManager.generateCompletion(
            "You are a helpful assistant",
            message,
            preferredModel  // "gpt-4" ou "llama2"
        );
    }
}
```

**Recursos Adicionais**:

Voc√™ pode consultar quais modelos est√£o dispon√≠veis:

```java
// Listar todos os modelos de todos os provedores
List<String> allModels = llmManager.getAllModels();
// Exemplo: ["llama2", "mistral", "gpt-3.5-turbo", "gpt-4"]

// Listar modelos por provedor
Map<Integer, List<String>> modelsByProvider = llmManager.getAllAvailableModels();
// {0: ["llama2", "mistral"], 1: ["gpt-3.5-turbo", "gpt-4"]}

// Descobrir qual provedor tem um modelo espec√≠fico
int providerIndex = llmManager.findProviderIndexByModel("gpt-4");
// Retorna: 1 (secondary)

// Obter o servi√ßo diretamente
LLMService service = llmManager.getServiceByModel("llama2");
```

**Vantagens**:
- ‚úÖ Roteamento transparente - n√£o precisa saber qual provedor tem qual modelo
- ‚úÖ Flexibilidade - f√°cil trocar entre modelos sem mudar c√≥digo
- ‚úÖ Suporte a partial matching - "gpt-4" encontra "gpt-4-turbo"
- ‚úÖ Fallback autom√°tico - usa primary se modelo n√£o encontrado
- ‚úÖ Case-insensitive - "GPT-4" == "gpt-4"

**Matching Inteligente**:
O sistema usa matching inteligente para encontrar modelos:
- Exact match: "gpt-4" == "gpt-4" ‚úÖ
- Contains: "gpt-4" encontra "gpt-4-turbo" ‚úÖ
- Partial: "llama" encontra "llama2" ‚úÖ
- Case-insensitive: "GPT-4" == "gpt-4" ‚úÖ

---

## Cen√°rios de Uso Comuns

### Cen√°rio 1: Alta Disponibilidade (Failover)

**Configura√ß√£o**:
```properties
# Primary: LM Studio local
llmservice.provider.name=LM_STUDIO
llmservice.provider.api.url=http://localhost:1234/v1

# Secondary: OpenAI backup
llmservice.provider2.enabled=true
llmservice.provider2.name=OPENAI
llmservice.provider2.api.key=${OPENAI_API_KEY}

# Estrat√©gia
llmservice.strategy=FAILOVER
```

**Comportamento**:
- Usa LM Studio local (r√°pido, gr√°tis)
- Se LM Studio estiver offline ‚Üí usa OpenAI automaticamente
- Logs informam sobre failover

**Vantagens**:
- M√°xima disponibilidade
- Custos m√≠nimos (s√≥ paga cloud quando necess√°rio)
- Sem downtime

---

### Cen√°rio 2: Distribui√ß√£o de Custos (Round-Robin)

**Configura√ß√£o**:
```properties
# Primary: Provider A (mais barato)
llmservice.provider.name=OLLAMA
llmservice.provider.embedding.model=llama2

# Secondary: Provider B (mais barato)
llmservice.provider2.enabled=true
llmservice.provider2.name=LM_STUDIO
llmservice.provider2.embedding.model=mistral

# Estrat√©gia
llmservice.strategy=ROUND_ROBIN
```

**Comportamento**:
- Alterna entre provedores
- Distribui rate limits
- Evita sobrecarga de um √∫nico provedor

**Vantagens**:
- Distribui custos
- Evita rate limits
- Load balancing

---

### Cen√°rio 3: H√≠brido Local + Cloud (Specialized)

**Configura√ß√£o**:
```properties
# Primary: Local para embeddings
llmservice.provider.name=LM_STUDIO
llmservice.provider.embedding.model=nomic-embed-text
llmservice.provider.embedding.dimension=768

# Secondary: Cloud para completions
llmservice.provider2.enabled=true
llmservice.provider2.name=OPENAI
llmservice.provider2.llm.models=gpt-4
llmservice.provider2.embedding.dimension=1536

# Estrat√©gia
llmservice.strategy=SPECIALIZED
```

**Comportamento**:
- Embeddings ‚Üí Local (r√°pido, gr√°tis, privado)
- Completions complexos ‚Üí Cloud (qualidade superior)

**Vantagens**:
- Performance + Qualidade
- Custos otimizados
- Privacidade de dados (embeddings locais)

---

### Cen√°rio 4: Quality Assurance (Dual Verification)

**Configura√ß√£o**:
```properties
# Primary: Model A
llmservice.provider.name=OPENAI
llmservice.provider.embedding.model=text-embedding-ada-002

# Secondary: Model B
llmservice.provider2.enabled=true
llmservice.provider2.name=COHERE
llmservice.provider2.embedding.model=embed-multilingual-v3.0

# Estrat√©gia
llmservice.strategy=DUAL_VERIFICATION
```

**Comportamento**:
- Executa em ambos os provedores
- Compara similaridade dos embeddings
- Alerta se diferen√ßa significativa

**Uso**:
- Valida√ß√£o de modelos
- Testes A/B
- Detec√ß√£o de anomalias

**Custo**: 2x (paga ambos os provedores)

---

### Cen√°rio 5: Roteamento por Modelo (Model-Based)

**Configura√ß√£o**:
```properties
# Primary: LM Studio com modelos locais
llmservice.provider.name=LM_STUDIO
llmservice.provider.llm.models=llama2,mistral-7b,qwen
llmservice.provider.embedding.model=nomic-embed-text
llmservice.provider.api.url=http://localhost:1234/v1

# Secondary: OpenAI com modelos cloud
llmservice.provider2.enabled=true
llmservice.provider2.name=OPENAI
llmservice.provider2.llm.models=gpt-3.5-turbo,gpt-4,gpt-4-turbo
llmservice.provider2.embedding.model=text-embedding-ada-002
llmservice.provider2.api.key=${OPENAI_API_KEY}

# Estrat√©gia
llmservice.strategy=MODEL_BASED
```

**Comportamento**:
- Request com `model="llama2"` ‚Üí Automaticamente usa LM Studio (Primary)
- Request com `model="gpt-4"` ‚Üí Automaticamente usa OpenAI (Secondary)
- Request com `model="unknown"` ‚Üí Fallback para Primary

**Vantagens**:
- Roteamento transparente
- Sem l√≥gica de if/else no c√≥digo
- F√°cil adicionar novos modelos
- Suporta m√∫ltiplos provedores com overlapping models

**Exemplo de Uso**:
```java
@RestController
public class ChatController {
    @Autowired
    private LLMServiceManager llmManager;

    @PostMapping("/chat")
    public String chat(@RequestParam String message,
                      @RequestParam(defaultValue = "llama2") String model) {
        // Roteamento autom√°tico baseado no modelo
        return llmManager.generateCompletion(
            "You are a helpful assistant",
            message,
            model
        );
    }

    @GetMapping("/models")
    public List<String> getAvailableModels() {
        // Lista todos os modelos dispon√≠veis
        return llmManager.getAllModels();
    }
}
```

**Quando Usar**:
- Aplica√ß√µes multi-tenant com prefer√™ncias de modelo por usu√°rio
- APIs que exp√µem m√∫ltiplos modelos aos clientes
- Ambientes com mix de modelos locais e cloud
- Desenvolvimento/teste com troca frequente de modelos

---

## Uso no C√≥digo

### Inje√ß√£o B√°sica

```java
@Service
public class SearchService {

    @Autowired
    private LLMServiceManager llmManager;

    public List<Document> search(String query) {
        // Gera embedding com estrat√©gia configurada
        float[] queryVector = llmManager.generateEmbedding(query);

        // Usa vector na busca
        return documentRepository.searchByVector(queryVector);
    }
}
```

### Com Tratamento de Erros

```java
@Service
public class DocumentProcessor {

    @Autowired
    private LLMServiceManager llmManager;

    public void processDocument(Document doc) {
        try {
            float[] embedding = llmManager.generateEmbedding(doc.getText());
            doc.setEmbedding(embedding);

        } catch (LLMServiceException e) {
            log.error("Failed to generate embedding: {}", e.getUserMessage());

            // Verifica tipo de erro
            switch (e.getErrorType()) {
                case RATE_LIMIT:
                    // Aguarda e tenta novamente
                    Thread.sleep(60000);
                    break;

                case AUTH_ERROR:
                    // Problema de configura√ß√£o
                    alertAdmin("API key invalid");
                    break;

                case SERVICE_UNAVAILABLE:
                    // Todos os provedores offline
                    queueForRetry(doc);
                    break;
            }
        }
    }
}
```

### Monitoramento de Estat√≠sticas

```java
@RestController
@RequestMapping("/api/v1/admin/llm")
public class LLMAdminController {

    @Autowired
    private LLMServiceManager llmManager;

    @GetMapping("/stats")
    public ResponseEntity<LLMServiceStats> getStats() {
        LLMServiceStats stats = llmManager.getStatistics();
        return ResponseEntity.ok(stats);
    }

    @PostMapping("/reset-stats")
    public ResponseEntity<Void> resetStats() {
        llmManager.resetStatistics();
        return ResponseEntity.noContent().build();
    }

    @GetMapping("/health")
    public ResponseEntity<Map<String, Boolean>> healthCheck() {
        Map<String, Boolean> health = new HashMap<>();
        health.put("primary", llmManager.isProviderHealthy(0));

        if (llmManager.getProviderCount() > 1) {
            health.put("secondary", llmManager.isProviderHealthy(1));
        }

        return ResponseEntity.ok(health);
    }
}
```

**Resposta de `/stats`**:
```json
{
  "providerCount": 2,
  "primaryRequests": 850,
  "secondaryRequests": 23,
  "failoverEvents": 12,
  "totalRequests": 873,
  "secondaryUsagePercentage": 2.6
}
```

---

## Provedores Suportados

| Provider | Embedding | Completion | Local | API Key | Dimens√µes Comuns |
|----------|-----------|------------|-------|---------|------------------|
| **LM_STUDIO** | ‚úÖ | ‚úÖ | ‚úÖ | ‚ùå | 768, 1024 |
| **OLLAMA** | ‚úÖ | ‚úÖ | ‚úÖ | ‚ùå | 768, 1024, 4096 |
| **OPENAI** | ‚úÖ | ‚úÖ | ‚ùå | ‚úÖ | 1536, 3072 |
| **ANTHROPIC** | ‚ùå | ‚úÖ | ‚ùå | ‚úÖ | - |
| **COHERE** | ‚úÖ | ‚úÖ | ‚ùå | ‚úÖ | 768, 1024, 4096 |
| **HUGGINGFACE** | ‚úÖ | ‚úÖ | ‚ùå | ‚úÖ | Varia |

---

## Exemplos de Configura√ß√£o por Caso de Uso

### Desenvolvimento Local (Gr√°tis)

```properties
llmservice.provider.name=LM_STUDIO
llmservice.provider2.enabled=false
llmservice.strategy=PRIMARY_ONLY
```

### Produ√ß√£o (Alta Disponibilidade)

```properties
# Primary: LM Studio
llmservice.provider.name=LM_STUDIO
llmservice.provider.api.url=http://llm-server:1234/v1

# Secondary: OpenAI backup
llmservice.provider2.enabled=true
llmservice.provider2.name=OPENAI
llmservice.provider2.api.key=${OPENAI_API_KEY}

llmservice.strategy=FAILOVER
llmservice.failover.max-retries=3
```

### Custo Otimizado

```properties
# Primary: Ollama local (gr√°tis)
llmservice.provider.name=OLLAMA
llmservice.provider.api.url=http://localhost:11434

# Secondary: OpenAI only for complex (pago)
llmservice.provider2.enabled=true
llmservice.provider2.name=OPENAI
llmservice.provider2.api.key=${OPENAI_API_KEY}

llmservice.strategy=SMART_ROUTING
```

### M√°xima Qualidade

```properties
# Primary: GPT-4
llmservice.provider.name=OPENAI
llmservice.provider.llm.models=gpt-4
llmservice.provider.api.key=${OPENAI_API_KEY}

# Secondary: Claude backup
llmservice.provider2.enabled=true
llmservice.provider2.name=ANTHROPIC
llmservice.provider2.llm.models=claude-3-opus
llmservice.provider2.api.key=${ANTHROPIC_API_KEY}

llmservice.strategy=FAILOVER
```

---

## Logs e Monitoramento

### Logs de Inicializa√ß√£o

```
INFO  MultiLLMServiceConfig - Initializing Primary LLMService
INFO  MultiLLMServiceConfig -   Provider: LM_STUDIO
INFO  MultiLLMServiceConfig -   Models: qwen/qwen3-1.7b
INFO  MultiLLMServiceConfig - Primary LLMService initialized successfully

INFO  MultiLLMServiceConfig - Initializing Secondary LLMService
INFO  MultiLLMServiceConfig -   Provider: OPENAI
INFO  MultiLLMServiceConfig - Secondary LLMService initialized successfully

INFO  MultiLLMServiceConfig - Initializing LLMServiceManager
INFO  MultiLLMServiceConfig -   Strategy: FAILOVER
INFO  MultiLLMServiceConfig -   Max Retries: 3
INFO  MultiLLMServiceConfig -   Total Providers: 2
INFO  MultiLLMServiceConfig - LLMServiceManager initialized with 2 provider(s)
```

### Logs de Failover

```
WARN  LLMServiceManager - Primary LLM service failed: Connection refused. Trying secondary...
INFO  LLMServiceManager - Successfully failed over to secondary provider
```

### Logs de Dual Verification

```
INFO  LLMServiceManager - Dual verification: Vector similarity = 0.94
WARN  LLMServiceManager - Dual verification: Low similarity (0.65)
```

---

## Troubleshooting

### Problema: Secondary n√£o est√° sendo usado

**Causa**: `llmservice.provider2.enabled=false`

**Solu√ß√£o**:
```properties
llmservice.provider2.enabled=true
```

### Problema: Failover n√£o funciona

**Verificar**:
1. Secondary habilitado?
2. Estrat√©gia √© FAILOVER?
3. Secondary configurado corretamente?

```bash
# Logs devem mostrar:
INFO - Initializing Secondary LLMService
```

### Problema: Custos muito altos

**Solu√ß√£o**: Use estrat√©gia SMART_ROUTING ou SPECIALIZED

```properties
llmservice.strategy=SMART_ROUTING
# Ou
llmservice.strategy=SPECIALIZED
```

### Problema: Lat√™ncia alta

**Causa**: Estrat√©gia DUAL_VERIFICATION

**Solu√ß√£o**: Mude para FAILOVER ou PRIMARY_ONLY

---

## Performance e Custos

### Compara√ß√£o de Estrat√©gias

| Estrat√©gia | Lat√™ncia | Custo | Disponibilidade | Uso | Flexibilidade |
|------------|----------|-------|-----------------|-----|---------------|
| PRIMARY_ONLY | ‚ö° Baixa | üí∞ M√≠nimo | ‚ö†Ô∏è M√©dia | Dev | ‚≠ê Baixa |
| FAILOVER | ‚ö° Baixa* | üí∞ Baixo | ‚úÖ Alta | Prod | ‚≠ê‚≠ê M√©dia |
| ROUND_ROBIN | ‚ö°‚ö° M√©dia | üí∞üí∞ M√©dio | ‚úÖ Alta | Scale | ‚≠ê‚≠ê M√©dia |
| SPECIALIZED | ‚ö° Vari√°vel | üí∞ Otimizado | ‚úÖ Alta | H√≠brido | ‚≠ê‚≠ê‚≠ê Alta |
| DUAL_VERIFICATION | üêå 2x | üí∞üí∞ 2x | ‚úÖ Alta | QA | ‚≠ê‚≠ê M√©dia |
| SMART_ROUTING | ‚ö° Baixa | üí∞ Otimizado | ‚úÖ Alta | Prod+ | ‚≠ê‚≠ê‚≠ê Alta |
| **MODEL_BASED** ‚≠ê | ‚ö° Baixa | üí∞ Vari√°vel | ‚úÖ Alta | Multi-Model | ‚≠ê‚≠ê‚≠ê‚≠ê Muito Alta |

*Baixa em opera√ß√£o normal, m√©dia durante failover

---

## Pr√≥ximos Passos

1. **M√©tricas Avan√ßadas**: Integra√ß√£o com Prometheus
2. **Circuit Breaker**: Desabilitar provedor com falhas frequentes
3. **Cache de Embeddings**: Evitar regenera√ß√£o
4. **Rate Limiting**: Controle de chamadas por provedor
5. **Dynamic Configuration**: Mudar estrat√©gia em runtime

---

**Preparado por**: Claude Code
**Data**: 2025-10-13
**Vers√£o**: 1.0
