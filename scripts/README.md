# Scripts de Setup - JSimpleRag

Esta pasta contÃ©m scripts para facilitar o setup e verificaÃ§Ã£o dos provedores LLM necessÃ¡rios para testes de integraÃ§Ã£o.

---

## ğŸ“œ Scripts DisponÃ­veis

### 1. setup-ollama.sh

**PropÃ³sito**: Instalar e configurar Ollama para testes de integraÃ§Ã£o

**O que faz**:
- âœ… Verifica se Ollama estÃ¡ instalado
- âœ… Instala Ollama se necessÃ¡rio (Linux/macOS)
- âœ… Inicia o servidor Ollama
- âœ… Baixa modelos necessÃ¡rios (tinyllama, nomic-embed-text)
- âœ… Verifica a configuraÃ§Ã£o
- âœ… Executa testes de conectividade

**Uso**:
```bash
./scripts/setup-ollama.sh
```

**Modelos instalados**:
- `tinyllama` (~600MB) - Modelo rÃ¡pido para testes
- `nomic-embed-text` (~274MB) - Embeddings
- `llama2` (opcional, ~3.8GB) - Modelo de melhor qualidade
- `mistral` (opcional, ~4.1GB) - Modelo de alta qualidade

**Requisitos**:
- Linux ou macOS
- ConexÃ£o com internet (para downloads)
- PermissÃµes sudo (para instalaÃ§Ã£o no Linux)

---

### 2. setup-lmstudio.sh

**PropÃ³sito**: Configurar LM Studio para testes de integraÃ§Ã£o

**O que faz**:
- âœ… Verifica se LM Studio estÃ¡ instalado
- âœ… Guia instalaÃ§Ã£o manual (Ã© uma GUI)
- âœ… Instrui iniciar o servidor local
- âœ… Guia download e carregamento de modelos
- âœ… Verifica a configuraÃ§Ã£o
- âœ… Executa testes de conectividade

**Uso**:
```bash
./scripts/setup-lmstudio.sh
```

**Modelos recomendados**:
- `qwen2.5-7b-instruct` (~4-5GB) - Modelo rÃ¡pido e eficiente
- `nomic-embed-text` (~274MB) - Embeddings

**Requisitos**:
- LM Studio instalado (baixe de https://lmstudio.ai)
- ConexÃ£o com internet (para downloads de modelos)
- ~10GB de espaÃ§o em disco

**Nota**: LM Studio Ã© uma aplicaÃ§Ã£o GUI e requer passos manuais.

---

### 3. check-providers.sh

**PropÃ³sito**: Verificar status dos provedores e testar conectividade

**O que faz**:
- âœ… Verifica se Ollama estÃ¡ rodando
- âœ… Verifica se LM Studio estÃ¡ rodando
- âœ… Lista modelos disponÃ­veis em cada provedor
- âœ… Valida modelos necessÃ¡rios
- âœ… Executa testes de conectividade (opcional)
- âœ… Mostra comandos Maven disponÃ­veis

**Uso**:
```bash
# Apenas verificar status
./scripts/check-providers.sh

# Verificar e testar conectividade
./scripts/check-providers.sh --test

# Mostrar ajuda
./scripts/check-providers.sh --help
```

**SaÃ­da exemplo**:
```
â–¶ Ollama (localhost:11434)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ… Server is running
  Version:                       0.1.28
  Models installed:              3

  â„¹ï¸  Available models:
    â€¢ tinyllama
    â€¢ llama2
    â€¢ nomic-embed-text

  â„¹ï¸  Required models check:
  âœ… tinyllama (required) âœ“
  âœ… nomic-embed-text (required) âœ“

â–¶ LM Studio (localhost:1234)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ… Server is running
  Models loaded:                 2

  â„¹ï¸  Loaded models:
    â€¢ qwen2.5-7b-instruct
    â€¢ nomic-embed-text

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âœ… ALL SYSTEMS GO!                                    â•‘
â•‘  Both providers are ready for integration tests       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… You can run: mvn verify -P integration-tests
```

---

## ğŸš€ Fluxo de Setup Recomendado

### Setup Inicial (primeira vez)

```bash
# 1. Setup Ollama (mais rÃ¡pido e essencial)
./scripts/setup-ollama.sh

# 2. Verificar se funcionou
./scripts/check-providers.sh

# 3. Rodar testes com Ollama apenas
cd ..
mvn verify -P integration-tests-ollama

# 4. (Opcional) Setup LM Studio para testes completos
./scripts/setup-lmstudio.sh

# 5. Verificar ambos os provedores
./scripts/check-providers.sh --test

# 6. Rodar testes completos
cd ..
mvn verify -P integration-tests
```

### VerificaÃ§Ã£o DiÃ¡ria

```bash
# Antes de rodar testes, verificar providers
./scripts/check-providers.sh

# Se algo nÃ£o estiver rodando, os scripts mostram como corrigir
```

---

## ğŸ¯ CenÃ¡rios de Uso

### CenÃ¡rio 1: Desenvolvimento Local (apenas Ollama)

**Quando**: Desenvolvimento diÃ¡rio, testes rÃ¡pidos

```bash
# Verificar
./scripts/check-providers.sh

# Se Ollama nÃ£o estiver rodando
ollama serve &

# Rodar testes
mvn verify -P integration-tests-ollama
```

**Tempo**: ~15 segundos

---

### CenÃ¡rio 2: Pre-Merge (Ollama + LM Studio)

**Quando**: Antes de fazer merge, validaÃ§Ã£o completa

```bash
# Verificar ambos
./scripts/check-providers.sh --test

# Se LM Studio nÃ£o estiver rodando:
# 1. Abrir LM Studio UI
# 2. Local Server â†’ Start Server

# Rodar testes completos
mvn verify -P integration-tests
```

**Tempo**: ~45 segundos

---

### CenÃ¡rio 3: CI/CD (GitHub Actions)

**Quando**: Pull requests, pipelines

```bash
# No CI, apenas Ollama
./scripts/setup-ollama.sh
mvn verify -P integration-tests-ollama
```

**Tempo**: ~20 segundos (com cache de modelos)

---

### CenÃ¡rio 4: Troubleshooting

**Problema**: Testes falhando

```bash
# 1. Verificar providers
./scripts/check-providers.sh --test

# 2. Se Ollama com problemas
./scripts/setup-ollama.sh

# 3. Se LM Studio com problemas
./scripts/setup-lmstudio.sh

# 4. Verificar logs
# Ollama: tail -f /tmp/ollama.log
# LM Studio: Verificar na UI
```

---

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### Alterar Porta do Ollama

```bash
# Iniciar Ollama em porta diferente
OLLAMA_HOST=0.0.0.0:11435 ollama serve &

# Ajustar nos scripts e properties
export OLLAMA_URL="http://localhost:11435"
```

### Usar Ollama Remoto

```bash
# No check-providers.sh, ajustar:
OLLAMA_URL="http://remote-server:11434"

# Em application-integration-test.properties:
llmservice.provider.api.url=http://remote-server:11434/v1
```

### Adicionar Novos Modelos

Edite `setup-ollama.sh`:
```bash
REQUIRED_MODELS=(
    "tinyllama"
    "nomic-embed-text"
    "seu-novo-modelo"  # Adicionar aqui
)
```

---

## ğŸ“Š ComparaÃ§Ã£o de Provedores

| Aspecto | Ollama | LM Studio |
|---------|--------|-----------|
| **InstalaÃ§Ã£o** | CLI (script automÃ¡tico) | GUI (manual) |
| **InicializaÃ§Ã£o** | `ollama serve` | BotÃ£o "Start Server" na UI |
| **Porta padrÃ£o** | 11434 | 1234 |
| **GestÃ£o modelos** | CLI (`ollama pull`) | UI (download na interface) |
| **Logs** | `/tmp/ollama.log` | Console na UI |
| **API** | OpenAI-compatible | OpenAI-compatible |
| **Uso em CI/CD** | âœ… Excelente | âŒ DifÃ­cil (requer GUI) |
| **Uso dev local** | âœ… Excelente | âœ… Excelente |

---

## ğŸ› Troubleshooting Comum

### "Connection refused" no Ollama

**Causa**: Servidor nÃ£o estÃ¡ rodando

**SoluÃ§Ã£o**:
```bash
# Verificar se estÃ¡ rodando
curl http://localhost:11434/api/tags

# Se nÃ£o, iniciar
ollama serve &

# Ou via systemd (Linux)
sudo systemctl start ollama
```

---

### "Connection refused" no LM Studio

**Causa**: Servidor nÃ£o estÃ¡ rodando na UI

**SoluÃ§Ã£o**:
1. Abrir LM Studio
2. Clicar em "â†”" (Local Server) na barra lateral
3. Clicar "Start Server"
4. Verificar se mostra "Server running on http://localhost:1234"

---

### Modelos nÃ£o encontrados

**Causa**: Modelos nÃ£o baixados ou nÃ£o carregados

**SoluÃ§Ã£o Ollama**:
```bash
# Baixar modelo
ollama pull tinyllama

# Verificar modelos instalados
ollama list
```

**SoluÃ§Ã£o LM Studio**:
1. Na UI, ir em "ğŸ”" (Search)
2. Buscar o modelo (ex: "qwen2.5")
3. Clicar "Download"
4. Aguardar download
5. Ir em "Local Server" e selecionar o modelo no dropdown
6. Clicar "Load Model"

---

### Erro "Out of Memory"

**Causa**: Modelo muito grande para a RAM disponÃ­vel

**SoluÃ§Ã£o**:
- Use modelos menores: `tinyllama` em vez de `llama2`
- Aumente swap do sistema
- Use quantizaÃ§Ã£o menor (ex: Q4 em vez de Q6)

---

## ğŸ“š ReferÃªncias

- [Ollama Documentation](https://github.com/ollama/ollama/blob/main/docs/README.md)
- [LM Studio Documentation](https://lmstudio.ai/docs)
- [Guia de Testes](../INTEGRATION_TEST_EXAMPLES.md)
- [Maven Profiles](../MAVEN_PROFILES_GUIDE.md)

---

## âœ… Checklist de Setup

- [ ] Ollama instalado
- [ ] Ollama rodando (`ollama serve`)
- [ ] Modelos Ollama baixados (tinyllama, nomic-embed-text)
- [ ] LM Studio instalado
- [ ] LM Studio servidor iniciado
- [ ] Modelos LM Studio carregados
- [ ] `check-providers.sh` passa âœ…
- [ ] `mvn verify -P integration-tests-ollama` passa âœ…
- [ ] `mvn verify -P integration-tests` passa âœ…

---

**Criado por**: Claude Code
**Data**: 2025-10-14
**VersÃ£o**: 1.0
